{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8eb03b",
   "metadata": {},
   "source": [
    "# Preprocessing and Word Association"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c42ab5",
   "metadata": {},
   "source": [
    "On this notebook is developed the preprocessing and word association with our corpus made by newspaper texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a6aef",
   "metadata": {},
   "source": [
    "## Preprocessing: Normalization\n",
    "\n",
    "First, we are going to define our functions to prepare our data before looking for word associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc791b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_corpus(path):\n",
    "    # Getting corpus from the directory\n",
    "    corpus = nltk.corpus.PlaintextCorpusReader(path, '.*')\n",
    "    file_list = corpus.fileids()\n",
    "    \n",
    "    # Reuniting all text content from files on the directory\n",
    "    all_text = ''\n",
    "    for file in file_list:\n",
    "        with open(path + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    \n",
    "    # Cleaning HTML tags\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa2c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_words(text):\n",
    "    words = text.split()\n",
    "    alphabetic_words = list()\n",
    "    \n",
    "    for word in words:\n",
    "        token = []\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "    \n",
    "    return alphabetic_words\n",
    "\n",
    "def tokenize_by_sents(text):\n",
    "    tokens = nltk.data.load(\"tokenizers/punkt/spanish.pickle\") \n",
    "    sents = tokens.tokenize(text)\n",
    "    alphabetic_sents = list()\n",
    "    \n",
    "    for sent in sents:\n",
    "        sent_token = tokenize_by_words(sent)\n",
    "        alphabetic_sents.append(sent_token)\n",
    "    \n",
    "    return alphabetic_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fd2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_from_words(words, path = './stopwords_es.txt'):\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]\n",
    "    \n",
    "    clean_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return clean_words\n",
    "    \n",
    "def remove_stop_words_from_sents(sents, path = './stopwords_es.txt'):\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]\n",
    "        \n",
    "    clean_sents = list()\n",
    "    for sent in sents:\n",
    "        clean_sent = [word for word in sent if word not in stop_words]\n",
    "        clean_sents.append(clean_sent)\n",
    "    \n",
    "    return clean_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b9f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_from_words(text, path = './generate.txt'):\n",
    "    from pickle import dump\n",
    "    lemmas = dict()\n",
    "    with open(path, encoding = 'latin1') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                lemmas[token] = lemma\n",
    "    \n",
    "    lemmatized_text = list()\n",
    "    for word in text:\n",
    "        if word in lemmas.keys():\n",
    "            lemmatized_text.append(lemmas[word])\n",
    "        else:\n",
    "            lemmatized_text.append(word)\n",
    "\n",
    "    return lemmatized_text\n",
    "    \n",
    "\n",
    "def lemmatize_from_sents(text, path = './generate.txt'):\n",
    "    from pickle import dump\n",
    "    lemmas = dict()\n",
    "    with open(path, encoding = 'latin1') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('#', '')\n",
    "                lemma = words[-1].strip()\n",
    "                lemmas[token] = lemma\n",
    "    \n",
    "    lemmatized_text = list()\n",
    "    for sent in text:\n",
    "        #words = sent.split()\n",
    "        lemmatized_sent = list()\n",
    "        for word in sent:\n",
    "            if word in lemmas.keys():\n",
    "                lemmatized_sent.append(lemmas[word])\n",
    "            else:\n",
    "                lemmatized_sent.append(word)\n",
    "        \n",
    "        lemmatized_text.append(lemmatized_sent)\n",
    "\n",
    "    return lemmatized_text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9835be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_words(path):\n",
    "    clean_text = extract_text_from_corpus(path)\n",
    "    alphabetic_words = tokenize_by_words(clean_text)     \n",
    "    clean_words = remove_stop_words_from_words(alphabetic_words)\n",
    "    preprocessed_text_words  = lemmatize_from_words(text = clean_words)\n",
    "    #print('\\n\\033[1mLemmatization from words completed\\033[0m')\n",
    "    #print(f'Some words after lemmatization: \\n{preprocessed_text_words[:200]}')\n",
    "    \n",
    "    return preprocessed_text_words\n",
    "\n",
    "def normalize_by_sents(path):\n",
    "    clean_text = extract_text_from_corpus(path)\n",
    "    alphabetic_sents = tokenize_by_sents(clean_text)     \n",
    "    clean_sents = remove_stop_words_from_sents(alphabetic_sents)\n",
    "    preprocessed_text_sents = lemmatize_from_sents(text = clean_sents)\n",
    "    #print('\\n\\033[1mLemmatization from sentence completed\\033[0m')\n",
    "    #print(f'Some words after lemmatization: \\n{preprocessed_text_sents[:200]}')\n",
    "    \n",
    "    return preprocessed_text_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "601e89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path = './../EXCELSIOR_100_files/', by_words = True, by_sents = False):\n",
    "    if by_words:\n",
    "        try:\n",
    "            preprocessed_text_words  = normalize_by_words(path)\n",
    "            print('\\033[1mNormalization by word tokens completed\\033[0m')\n",
    "            #print(f'Some words after normalization: \\n{clean_words[:200]}')\n",
    "            \n",
    "            return preprocessed_text_words\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('An error has occured: ', e)\n",
    "    elif by_sents:\n",
    "        try:\n",
    "            preprocessed_text_sents = normalize_by_sents(path)\n",
    "            print('\\033[1mNormalization by sentence tokens completed\\033[0m')\n",
    "            #print(f'Some words after normalization: \\n{clean_sents[:200]}')\n",
    "\n",
    "            return preprocessed_text_sents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('An error has occured: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236024cf",
   "metadata": {},
   "source": [
    "#### Preprocessing with word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5454df1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNormalization by word tokens completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text_words = preprocessing(by_words = True, by_sents = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc78f24c",
   "metadata": {},
   "source": [
    "#### Preprocessing with sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb13d17d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNormalization by sentence tokens completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text_sents = preprocessing(by_words = False, by_sents = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c15595",
   "metadata": {},
   "source": [
    "Besides, we are going to make a vocabulary from the preprocessed text made with word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "064e257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary(words):\n",
    "    vocabulary = sorted(list(set(words)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42fbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = make_vocabulary(preprocessed_text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c72bf891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5471"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c0cda7",
   "metadata": {},
   "source": [
    "## Word associations\n",
    "\n",
    "Once we have completed our preprocessing, it's time to define the functions to find the similarity between words, hence get the word associations in our corpus. In order to do that, we must have functions to extract the contexts from both text preprocessed gotten with words and sentences. Then, we also need a function to calculate the probability's vector for each word. And finally, functions to quantify the similarity between words based on the dot product or the cosine measure between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f60fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts_words(vocabulary, text, window = 8):\n",
    "    contexts = dict()\n",
    "    for w in vocabulary:\n",
    "        context = list()\n",
    "        for i in range(len(text)):\n",
    "            if text[i] == w:\n",
    "                for j in range(i - int(window / 2), i):\n",
    "                    if j >= 0:\n",
    "                        context.append(text[j])\n",
    "                try: \n",
    "                    for j in range(i + 1, i + (int(window / 2) + 1)):\n",
    "                        context.append(text[j])\n",
    "                except IndexError:\n",
    "                        pass\n",
    "        contexts[w] = context\n",
    "    return contexts\n",
    "\n",
    "\n",
    "def get_contexts_sents(vocabulary, text, window = 8):\n",
    "    contexts = dict()\n",
    "    for w in vocabulary:\n",
    "        context = list()\n",
    "        for sent in text:\n",
    "            words = sent\n",
    "            for i in range(len(words)):\n",
    "                if words[i] == w:\n",
    "                    for j in range(i - int(window / 2), i):\n",
    "                        if j >= 0:\n",
    "                            context.append(words[j])\n",
    "                    try:\n",
    "                        for j in range(i + 1, i + (int(window / 2) + 1)):\n",
    "                            context.append(words[j])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "        contexts[w] = context\n",
    "    \n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b73d5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vectors(vocabulary, contexts):\n",
    "    probs = dict()\n",
    "    for v in vocabulary:\n",
    "        context = contexts[v]\n",
    "        vector_normalized = []\n",
    "        for voc in vocabulary:\n",
    "            vector_normalized.append(context.count(voc))\n",
    "        vector_normalized = np.array(vector_normalized)\n",
    "        s = np.sum(vector_normalized)\n",
    "        if s != 0:\n",
    "            vector_normalized = vector_normalized / s\n",
    "        probs[v] = vector_normalized\n",
    "    return probs\n",
    "\n",
    "\n",
    "def s_dot_product(word, vectors, aux_path = ''):\n",
    "\n",
    "    similarities = dict()\n",
    "    v = vectors[word]\n",
    "    for w in vectors.keys():\n",
    "        similarities[w] = np.dot(vectors[w], v)\n",
    "    similarities = (sorted(similarities.items(), key = lambda item: item[1], reverse = True))\n",
    "    \n",
    "    with open('./dot_product/similar_words_to_' + word + '_with_dot_product_of_' + aux_path + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        for item in similarities:\n",
    "            f.write(str(item) + '\\n')\n",
    "\n",
    "\n",
    "def s_cosine(word, vectors, aux_path = ''):\n",
    "    similarities = dict()\n",
    "    v = vectors[word]\n",
    "    for w in vectors.keys():\n",
    "        v2 = vectors[w]\n",
    "        norm1 = np.linalg.norm(v)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            similarities[w] = 0\n",
    "        else:\n",
    "            similarities[w] = np.dot(v, v2) / (norm1 * norm2)\n",
    "    similarities = (sorted(similarities.items(), key = lambda item: item[1], reverse = True))\n",
    "    \n",
    "    with open('./cosine/similar_words_to_' + word + '_with_cosine_of_' + aux_path + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        for item in similarities:\n",
    "            f.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795085e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(word, vectors, aux_path = '', tf_idf = False, dot_product = False, cosine = False):   \n",
    " \n",
    "    if dot_product:\n",
    "        s_dot_product(word, vectors, aux_path)\n",
    "                \n",
    "    if cosine:\n",
    "        s_cosine(word, vectors, aux_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a201d6",
   "metadata": {},
   "source": [
    "#### Word associations from preprocessed text with word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb930012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity estimations of \u001b[1mempresa\u001b[0m completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contexts_w = get_contexts_words(vocabulary, preprocessed_text_words)\n",
    "probs_w = get_vectors(vocabulary, contexts_w)\n",
    "w = \"empresa\"\n",
    "try:\n",
    "    similar_words(w, probs_w, aux_path = 'prob_vectors_by_words', dot_product = True, cosine = True)\n",
    "    print('Similarity estimations of \\033[1m' + w + '\\033[0m completed\\n')\n",
    "except Exception as e:\n",
    "    print('An error has occured: ' + e + ' in word ' + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f1576",
   "metadata": {},
   "source": [
    "#### Word associations from preprocessed text with sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e800b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity estimations of \u001b[1mempresa\u001b[0m completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contexts_s = get_contexts_sents(vocabulary, preprocessed_text_sents)\n",
    "probs_s = get_vectors(vocabulary, contexts_s)\n",
    "w = \"empresa\"\n",
    "try:\n",
    "    similar_words(w, probs_s, aux_path = 'prob_vectors_by_sents', dot_product = True, cosine = True)\n",
    "    print('Similarity estimations of \\033[1m' + w + '\\033[0m completed\\n')\n",
    "except Exception as e:\n",
    "    print('An error has occured: ' + e + ' in word ' + w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
